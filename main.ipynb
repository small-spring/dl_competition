{"cells":[{"cell_type":"markdown","metadata":{},"source":["# パッケージ・パス"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":783,"status":"ok","timestamp":1716816175306,"user":{"displayName":"三富佑人","userId":"02381481834375845603"},"user_tz":-540},"id":"6MK4bTe6NbUx"},"outputs":[],"source":["# パスへの移動\n","import os\n","project_path = '/workspace/'\n","os.chdir(project_path)\n","\n","\n","# パッケージのロード\n","import torch\n","import hydra\n","from omegaconf import DictConfig\n","from torch.utils.data import DataLoader\n","import random\n","import numpy as np\n","from src.models.evflownet import EVFlowNet\n","from src.datasets import DatasetProvider\n","from enum import Enum, auto\n","from src.datasets import train_collate\n","from tqdm import tqdm\n","from pathlib import Path\n","from typing import Dict, Any\n","import os\n","import time"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"executionInfo":{"elapsed":6803,"status":"ok","timestamp":1716816183753,"user":{"displayName":"三富佑人","userId":"02381481834375845603"},"user_tz":-540},"id":"hcZTkQcZYICb","outputId":"41156112-c8a8-4b50-e0de-ea307d2031f9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: hydra-core in /home/ubuntu/.local/lib/python3.10/site-packages (1.3.2)\n","Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ubuntu/.local/lib/python3.10/site-packages (from hydra-core) (4.9.3)\n","Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.10/site-packages (from hydra-core) (24.0)\n","Requirement already satisfied: omegaconf<2.4,>=2.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from hydra-core) (2.3.0)\n","Requirement already satisfied: PyYAML>=5.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.1)\n","Defaulting to user installation because normal site-packages is not writeable\n","Requirement already satisfied: hdf5plugin in /home/ubuntu/.local/lib/python3.10/site-packages (4.4.0)\n","Requirement already satisfied: h5py in /home/ubuntu/.local/lib/python3.10/site-packages (from hdf5plugin) (3.11.0)\n","Requirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from h5py->hdf5plugin) (1.26.4)\n","Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n","Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n","Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease               \n","Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n","Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Reading package lists... Done\n"]}],"source":["!pip install hydra-core --upgrade\n","!pip install hdf5plugin\n","!sudo apt-get update\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9427393,"status":"ok","timestamp":1716825748347,"user":{"displayName":"三富佑人","userId":"02381481834375845603"},"user_tz":-540},"id":"0KDGC846Pk7f","outputId":"af29170b-db2b-4fc2-c5e1-7d4f36bb4ab9"},"outputs":[],"source":["# %env HYDRA_FULL_ERROR=1\n","# !python /workspace/main.py"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dataset_path: data\n","seed: 42\n","num_epoch: 10\n","data_loader:\n","  common:\n","    num_voxel_bins: 15\n","  train:\n","    batch_size: 8\n","    shuffle: false\n","  test:\n","    batch_size: 1\n","    shuffle: false\n","train:\n","  no_batch_norm: false\n","  initial_learning_rate: 0.01\n","  weight_decay: 0.0001\n","  epochs: 10\n","\n"]}],"source":["# hydra用のyamlファイルを読んでargsにロードする。\n","\n","import os\n","from hydra import initialize, initialize_config_module, initialize_config_dir, compose\n","from omegaconf import OmegaConf\n","\n","with initialize_config_dir(version_base=None, config_dir=\"/workspace/configs\"):\n","    args = compose(config_name=\"base\")\n","\n","print(OmegaConf.to_yaml(args))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# 関数の定義\n","class RepresentationType(Enum):\n","    VOXEL = auto()\n","    STEPAN = auto()\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","\n","def compute_epe_error(pred_flow: torch.Tensor, gt_flow: torch.Tensor):\n","    '''\n","    end-point-error (ground truthと予測値の二乗誤差)を計算\n","    pred_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 予測したオプティカルフローデータ\n","    gt_flow: torch.Tensor, Shape: torch.Size([B, 2, 480, 640]) => 正解のオプティカルフローデータ\n","    '''\n","    epe = torch.mean(torch.mean(torch.norm(pred_flow - gt_flow, p=2, dim=1), dim=(1, 2)), dim=0)\n","    return epe\n","\n","def save_optical_flow_to_npy(flow: torch.Tensor, file_name: str):\n","    '''\n","    optical flowをnpyファイルに保存\n","    flow: torch.Tensor, Shape: torch.Size([2, 480, 640]) => オプティカルフローデータ\n","    file_name: str => ファイル名\n","    '''\n","    np.save(f\"{file_name}.npy\", flow.cpu().numpy())\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["'\\ntrain data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\\n    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\\n\\ntest data:\\n    Type of batch: Dict\\n    Key: seq_name, Type: list\\n    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\\n'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# モデルの保存ディレクトリの作成・データのロード。\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists('checkpoints'):\n","    os.makedirs('checkpoints')\n","\n","set_seed(args.seed)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","'''\n","    ディレクトリ構造:\n","\n","    data\n","    ├─test\n","    |  ├─test_city\n","    |  |    ├─events_left\n","    |  |    |   ├─events.h5\n","    |  |    |   └─rectify_map.h5\n","    |  |    └─forward_timestamps.txt\n","    └─train\n","        ├─zurich_city_11_a\n","        |    ├─events_left\n","        |    |       ├─ events.h5\n","        |    |       └─ rectify_map.h5\n","        |    ├─ flow_forward\n","        |    |       ├─ 000134.png\n","        |    |       |.....\n","        |    └─ forward_timestamps.txt\n","        ├─zurich_city_11_b\n","        └─zurich_city_11_c\n","    '''\n","\n","# ------------------\n","#    Dataloader\n","# ------------------\n","loader = DatasetProvider(\n","    dataset_path=Path(args.dataset_path),\n","    representation_type=RepresentationType.VOXEL,\n","    delta_t_ms=100,\n","    num_bins=4\n",")\n","train_set = loader.get_train_dataset()\n","test_set = loader.get_test_dataset()\n","collate_fn = train_collate\n","\n","\n","train_data = DataLoader(train_set,\n","                                batch_size=args.data_loader.train.batch_size,\n","                                shuffle=args.data_loader.train.shuffle,\n","                                collate_fn=collate_fn,\n","                                drop_last=False)\n","test_data = DataLoader(test_set,\n","                                batch_size=args.data_loader.test.batch_size,\n","                                shuffle=args.data_loader.test.shuffle,\n","                                collate_fn=collate_fn,\n","                                drop_last=False)\n","\n","'''\n","train data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","    Key: flow_gt, Type: torch.Tensor, Shape: torch.Size([Batch, 2, 480, 640]) => オプティカルフローデータのバッチ\n","    Key: flow_gt_valid_mask, Type: torch.Tensor, Shape: torch.Size([Batch, 1, 480, 640]) => オプティカルフローデータのvalid. ベースラインでは使わない\n","\n","test data:\n","    Type of batch: Dict\n","    Key: seq_name, Type: list\n","    Key: event_volume, Type: torch.Tensor, Shape: torch.Size([Batch, 4, 480, 640]) => イベントデータのバッチ\n","'''"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([4, 480, 640])"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["train_set[0]['event_volume'].size()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["def save_model(model):\n","    current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","    model_path = f\"checkpoints/model_{current_time}.pth\"\n","    torch.save(model.state_dict(), model_path)\n","    print(f\"Model saved to {model_path}\")\n","\n","    # ------------------\n","    #   Start predicting ()\n","    # ------------------\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","    flow: torch.Tensor = torch.tensor([]).to(device)\n","    with torch.no_grad():\n","        print(\"start test\")\n","        for batch in tqdm(test_data):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device)\n","            batch_flow = model(event_image) # [1, 2, 480, 640]\n","            flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","        print(\"test done\")\n","    # ------------------\n","    #  save submission\n","    # ------------------\n","    file_name = \"submission\"\n","    save_optical_flow_to_npy(flow, file_name)\n","    print(\"Submission saved\")\n","\n","\n","def train_model(args, model, n_epoch=1):\n","    # ------------------\n","    #   optimizer\n","    # ------------------\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.train.initial_learning_rate, weight_decay=args.train.weight_decay)\n","\n","    # ------------------\n","    #   Start training\n","    # ------------------\n","    model.train()\n","    for epoch in range(n_epoch):\n","        total_loss = 0\n","        print(\"on epoch: {}\".format(epoch+1))\n","        for i, batch in enumerate(tqdm(train_data)):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device) # [B, 4, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","            flow = model(event_image) # [B, 2, 480, 640]\n","            loss: torch.Tensor = compute_epe_error(flow, ground_truth_flow)\n","            print(f\"batch {i} loss: {loss.item()}\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n","        \n","        current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","        model_path = f\"checkpoints/model_{current_time}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","        print(f\"Model saved to {model_path}\")\n","\n","\n","    # ------------------\n","    #   Start predicting ()\n","    # ------------------\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","    flow: torch.Tensor = torch.tensor([]).to(device)\n","    with torch.no_grad():\n","        print(\"start test\")\n","        for batch in tqdm(test_data):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device)\n","            batch_flow = model(event_image) # [1, 2, 480, 640]\n","            flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","        print(\"test done\")\n","    # ------------------\n","    #  save submission\n","    # ------------------\n","    file_name = \"submission\"\n","    save_optical_flow_to_npy(flow, file_name)"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["def train_model(args, model, n_epoch=1):\n","    # ------------------\n","    #   optimizer\n","    # ------------------\n","    optimizer = torch.optim.Adam(model.parameters(), lr=args.train.initial_learning_rate, weight_decay=args.train.weight_decay)\n","\n","    # ------------------\n","    #   Start training\n","    # ------------------\n","    model.train()\n","    for epoch in range(n_epoch):\n","        total_loss = 0\n","        print(\"on epoch: {}\".format(epoch+1))\n","        for i, batch in enumerate(tqdm(train_data)):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device) # [B, 4, 480, 640]\n","            ground_truth_flow = batch[\"flow_gt\"].to(device) # [B, 2, 480, 640]\n","            flow = model(event_image) # [B, 2, 480, 640]\n","            loss: torch.Tensor = compute_epe_error(flow, ground_truth_flow)\n","            print(f\"batch {i} loss: {loss.item()}\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            if loss < 2.5:\n","                torch.save(model.state_dict(), \"checkpoints/model_tmp.pth\")\n","                print(\"tmp model saved!\")\n","\n","\n","            total_loss += loss.item()\n","        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data)}')\n","        \n","        current_time = time.strftime(\"%Y%m%d%H%M%S\")\n","        model_path = f\"checkpoints/model_{current_time}.pth\"\n","        torch.save(model.state_dict(), model_path)\n","        print(f\"Model saved to {model_path}\")\n","\n","\n","    # ------------------\n","    #   Start predicting ()\n","    # ------------------\n","    model.load_state_dict(torch.load(model_path, map_location=device))\n","    model.eval()\n","    flow: torch.Tensor = torch.tensor([]).to(device)\n","    with torch.no_grad():\n","        print(\"start test\")\n","        for batch in tqdm(test_data):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device)\n","            batch_flow = model(event_image) # [1, 2, 480, 640]\n","            flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","        print(\"test done\")\n","    # ------------------\n","    #  save submission\n","    # ------------------\n","    file_name = \"submission\"\n","    save_optical_flow_to_npy(flow, file_name)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def evaluate(model, file_name=\"submission\"):\n","    # ------------------\n","    #   Start predicting ()\n","    # ------------------\n","    model.load_state_dict(torch.load(\"checkpoints/model_tmp.pth\", map_location=device))\n","    model.eval()\n","    flow: torch.Tensor = torch.tensor([]).to(device)\n","    with torch.no_grad():\n","        print(\"start test\")\n","        for batch in tqdm(test_data):\n","            batch: Dict[str, Any]\n","            event_image = batch[\"event_volume\"].to(device)\n","            batch_flow = model(event_image) # [1, 2, 480, 640]\n","            flow = torch.cat((flow, batch_flow), dim=0)  # [N, 2, 480, 640]\n","        print(\"test done\")\n","    # ------------------\n","    #  save submission\n","    # ------------------\n","    save_optical_flow_to_npy(flow, file_name)"]},{"cell_type":"markdown","metadata":{},"source":["# 2d ver."]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["on epoch: 1\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/252 [00:00<?, ?it/s]"]},{"name":"stdout","output_type":"stream","text":["batch 0 loss: 5.663653426153224\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 1/252 [00:24<1:40:30, 24.02s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 1 loss: 9.096436697539687\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 2/252 [00:48<1:40:46, 24.18s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 2 loss: 16.774764216209267\n"]},{"name":"stderr","output_type":"stream","text":["  1%|          | 3/252 [01:11<1:38:44, 23.79s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 3 loss: 4.297264297191079\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 4/252 [01:34<1:37:22, 23.56s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 4 loss: 3.9926119926452897\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 5/252 [01:58<1:37:33, 23.70s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 5 loss: 2.9989616225109317\n"]},{"name":"stderr","output_type":"stream","text":["  2%|▏         | 6/252 [02:23<1:37:54, 23.88s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 6 loss: 2.839137585535814\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 7/252 [02:46<1:36:56, 23.74s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 7 loss: 2.878798417885876\n"]},{"name":"stderr","output_type":"stream","text":["  3%|▎         | 8/252 [03:10<1:36:38, 23.77s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 8 loss: 2.8858405702379626\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▎         | 9/252 [03:33<1:35:45, 23.64s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 9 loss: 3.413568429723508\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 10/252 [03:57<1:35:32, 23.69s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 10 loss: 3.0218185383264835\n"]},{"name":"stderr","output_type":"stream","text":["  4%|▍         | 11/252 [04:21<1:35:44, 23.84s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 11 loss: 2.558471203201143\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▍         | 12/252 [04:45<1:35:40, 23.92s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 12 loss: 3.6351077001823517\n"]},{"name":"stderr","output_type":"stream","text":["  5%|▌         | 13/252 [05:08<1:34:17, 23.67s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 13 loss: 3.788629974202085\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 14/252 [05:32<1:33:54, 23.67s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 14 loss: 8.196597770160722\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▌         | 15/252 [05:56<1:33:55, 23.78s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 15 loss: 3.5964874247314067\n"]},{"name":"stderr","output_type":"stream","text":["  6%|▋         | 16/252 [06:20<1:33:33, 23.78s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 16 loss: 3.228619726406445\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 17/252 [06:43<1:32:59, 23.74s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 17 loss: 7.997601200328916\n"]},{"name":"stderr","output_type":"stream","text":["  7%|▋         | 18/252 [07:08<1:32:58, 23.84s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 18 loss: 4.869118022306216\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 19/252 [07:31<1:32:30, 23.82s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 19 loss: 5.055208189957071\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 20/252 [07:55<1:31:59, 23.79s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 20 loss: 7.1758264523021005\n"]},{"name":"stderr","output_type":"stream","text":["  8%|▊         | 21/252 [08:18<1:31:04, 23.65s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 21 loss: 7.0196266048759774\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▊         | 22/252 [08:42<1:30:29, 23.61s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 22 loss: 5.00538333069028\n"]},{"name":"stderr","output_type":"stream","text":["  9%|▉         | 23/252 [09:05<1:30:04, 23.60s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 23 loss: 3.7920572387137605\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|▉         | 24/252 [09:29<1:29:55, 23.66s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 24 loss: 8.522993891265052\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|▉         | 25/252 [09:53<1:29:43, 23.72s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 25 loss: 4.071835188830104\n"]},{"name":"stderr","output_type":"stream","text":[" 10%|█         | 26/252 [10:17<1:28:59, 23.62s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 26 loss: 4.461844003396163\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 27/252 [10:40<1:27:51, 23.43s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 27 loss: 4.218791676562974\n"]},{"name":"stderr","output_type":"stream","text":[" 11%|█         | 28/252 [11:04<1:28:14, 23.64s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 28 loss: 3.6956655799057962\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 29/252 [11:27<1:28:03, 23.69s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 29 loss: 7.577784088225249\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 30/252 [11:51<1:27:44, 23.71s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 30 loss: 6.891739449635802\n"]},{"name":"stderr","output_type":"stream","text":[" 12%|█▏        | 31/252 [12:15<1:27:07, 23.65s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 31 loss: 4.309045923478886\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 32/252 [12:38<1:26:42, 23.65s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 32 loss: 5.681415197376641\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 33/252 [13:02<1:26:16, 23.64s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 33 loss: 3.2318092493513197\n"]},{"name":"stderr","output_type":"stream","text":[" 13%|█▎        | 34/252 [13:26<1:25:46, 23.61s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 34 loss: 2.9999616491171412\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 35/252 [13:49<1:25:33, 23.66s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 35 loss: 2.505910030229362\n"]},{"name":"stderr","output_type":"stream","text":[" 14%|█▍        | 36/252 [14:12<1:24:29, 23.47s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 36 loss: 2.730997918729463\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▍        | 37/252 [14:35<1:23:43, 23.37s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 37 loss: 6.819557102964949\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 38/252 [15:00<1:24:44, 23.76s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 38 loss: 9.503727396688674\n"]},{"name":"stderr","output_type":"stream","text":[" 15%|█▌        | 39/252 [15:25<1:25:44, 24.15s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 39 loss: 11.353917759523476\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 40/252 [15:50<1:26:17, 24.42s/it]"]},{"name":"stdout","output_type":"stream","text":["batch 40 loss: 4.932374280968431\n"]},{"name":"stderr","output_type":"stream","text":[" 16%|█▌        | 40/252 [16:15<1:26:07, 24.38s/it]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m EVFlowNet(args\u001b[38;5;241m.\u001b[39mtrain)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path_load, map_location\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m evaluate(model)\n","Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(args, model, n_epoch)\u001b[0m\n\u001b[1;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2.5\u001b[39m:\n\u001b[1;32m     25\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpoints/model_tmp.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtmp model saved!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# model_path_loadのモデルをロード\n","model_path_load=\"/workspace/checkpoints/model_20240717025440.pth\"\n","model = EVFlowNet(args.train).to(device)\n","model.load_state_dict(torch.load(model_path_load, map_location=device))\n","train_model(args, model)\n","\n","evaluate(model)"]},{"cell_type":"markdown","metadata":{},"source":["# Mine version"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# model = EVFlowNetMy(args.train).to(device)\n","# model.load_state_dict(torch.load(\"checkpoints/model_tmp.pth\", map_location=device))\n","# evaluate(model, \"submission_tmp\")\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Error(s) in loading state_dict for EVFlowNetMy:\n\tsize mismatch for decoder4.general_conv2d.0.weight: copying a param with shape torch.Size([32, 130, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 130, 3, 3]).\n\tsize mismatch for decoder4.general_conv2d.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.predict_flow.0.weight: copying a param with shape torch.Size([2, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 16, 1, 1]).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_path_load\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/workspace/checkpoints/model_20240717020629.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m EVFlowNetMy(args\u001b[38;5;241m.\u001b[39mtrain)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path_load\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m train_model(args, model)\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n","\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EVFlowNetMy:\n\tsize mismatch for decoder4.general_conv2d.0.weight: copying a param with shape torch.Size([32, 130, 3, 3]) from checkpoint, the shape in current model is torch.Size([16, 130, 3, 3]).\n\tsize mismatch for decoder4.general_conv2d.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.running_mean: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.general_conv2d.2.running_var: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([16]).\n\tsize mismatch for decoder4.predict_flow.0.weight: copying a param with shape torch.Size([2, 32, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 16, 1, 1])."]}],"source":["from src.models.evflownet_my import EVFlowNetMy\n","\n","# model_path_loadのモデルをロード\n","model_path_load=\"/workspace/checkpoints/model_20240717020629.pth\"\n","model = EVFlowNetMy(args.train).to(device)\n","model.load_state_dict(torch.load(model_path_load, map_location=device))\n","train_model(args, model)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model saved to checkpoints/model_20240717052428.pth\n","start test\n"]},{"name":"stderr","output_type":"stream","text":["  0%|          | 0/97 [00:00<?, ?it/s]/home/ubuntu/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n","  return F.conv2d(input, weight, bias, self.stride,\n","100%|██████████| 97/97 [00:12<00:00,  7.97it/s]\n"]},{"name":"stdout","output_type":"stream","text":["test done\n","Submission saved\n"]}],"source":["save_model(model)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOwK7NsK3TpYBxd1M/JLLAI","gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
